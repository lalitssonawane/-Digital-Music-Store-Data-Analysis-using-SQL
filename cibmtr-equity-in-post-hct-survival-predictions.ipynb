{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":70942,"databundleVersionId":10381525,"sourceType":"competition"}],"dockerImageVersionId":30804,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:40:19.971131Z","iopub.execute_input":"2024-12-11T12:40:19.971396Z","iopub.status.idle":"2024-12-11T12:40:20.370129Z","shell.execute_reply.started":"2024-12-11T12:40:19.971368Z","shell.execute_reply":"2024-12-11T12:40:20.368744Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<p style=\"font-family: 'Amiri'; font-size: 3rem; color: white; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #2b8a3e; padding: 20px; border-radius: 20px; border: 7px solid #f1c40f; width:95%\">\r\n ｡CIBMTR - Equity in Post-HCT Survival Prediction˚\r\n</p>\r\n>\r\n","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:40:20.373076Z","iopub.execute_input":"2024-12-11T12:40:20.373691Z","iopub.status.idle":"2024-12-11T12:40:20.943169Z","shell.execute_reply.started":"2024-12-11T12:40:20.373638Z","shell.execute_reply":"2024-12-11T12:40:20.942215Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import plotly.express as px\nfrom IPython.display import display, HTML\nimport warnings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:40:20.944794Z","iopub.execute_input":"2024-12-11T12:40:20.945293Z","iopub.status.idle":"2024-12-11T12:40:21.238397Z","shell.execute_reply.started":"2024-12-11T12:40:20.945258Z","shell.execute_reply":"2024-12-11T12:40:21.237304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from colorama import Fore, Style\nfrom catboost import CatBoostClassifier\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer, OneHotEncoder, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, KFold\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, roc_auc_score\nfrom lightgbm import LGBMClassifier","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:40:21.239744Z","iopub.execute_input":"2024-12-11T12:40:21.240282Z","iopub.status.idle":"2024-12-11T12:40:22.176986Z","shell.execute_reply.started":"2024-12-11T12:40:21.240247Z","shell.execute_reply":"2024-12-11T12:40:22.175932Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from catboost import CatBoostRegressor\nfrom sklearn.model_selection import KFold\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:40:22.178297Z","iopub.execute_input":"2024-12-11T12:40:22.178923Z","iopub.status.idle":"2024-12-11T12:40:22.184455Z","shell.execute_reply.started":"2024-12-11T12:40:22.178872Z","shell.execute_reply":"2024-12-11T12:40:22.183293Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Importing dataset","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/equity-post-HCT-survival-predictions/train.csv')\ntest = pd.read_csv('/kaggle/input/equity-post-HCT-survival-predictions/test.csv')\ntrain_solution=train[['ID','efs','efs_time','race_group']].copy()\nsolution = pd.read_csv('/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:40:22.186015Z","iopub.execute_input":"2024-12-11T12:40:22.186336Z","iopub.status.idle":"2024-12-11T12:40:22.494430Z","shell.execute_reply.started":"2024-12-11T12:40:22.186305Z","shell.execute_reply":"2024-12-11T12:40:22.493186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from lifelines import KaplanMeierFitter\n\ndef transform_survival_probability(df, time_col='efs_time', event_col='efs'):\n    \"\"\"\n    Transforms the survival probabilities using Kaplan-Meier estimator.\n    \n    Parameters:\n        df (pd.DataFrame): The dataframe containing the survival data.\n        time_col (str): The name of the column containing time data.\n        event_col (str): The name of the column containing event/censoring information.\n    \n    Returns:\n        pd.Series: A series of survival probabilities.\n    \"\"\"\n    kmf = KaplanMeierFitter()\n    \n    # Fit Kaplan-Meier estimator\n    kmf.fit(df[time_col], event_observed=df[event_col])\n    \n    # Get survival probabilities at each time point\n    survival_probabilities = kmf.survival_function_at_times(df[time_col]).values.flatten()\n    \n    return survival_probabilities\n\n# Apply transformation to the training dataset\ntrain[\"target\"] = transform_survival_probability(train)\n\n# Columns to drop\ndrop_cols = [\"ID\", 'efs', 'efs_time']\n\n# Drop the columns from both training and test datasets\ntrain.drop(columns=drop_cols, inplace=True, errors='ignore')\ntest.drop(columns=drop_cols, inplace=True, errors='ignore')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:40:22.498386Z","iopub.execute_input":"2024-12-11T12:40:22.498797Z","iopub.status.idle":"2024-12-11T12:40:22.563506Z","shell.execute_reply.started":"2024-12-11T12:40:22.498731Z","shell.execute_reply":"2024-12-11T12:40:22.562352Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Peak data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Function to print styled heading\ndef print_heading(text, color=\"yellow\"):\n    print(f\"\\033[1;33;40m{'='*50}\")\n    print(f\"{text.center(50)}\")\n    print(f\"{'='*50}\\033[0m\")\n\n# Helper function for error messages\ndef print_error(message):\n    print(f\"\\033[1;31;40m{'='*50}\")\n    print(f\"Error: {message}\")\n    print(f\"{'='*50}\\033[0m\")\n\n# Function to display dataset summary\ndef display_dataset_summary(dataset, dataset_name=\"Dataset\"):\n    try:\n        print_heading(f\"{dataset_name} Shape\")\n        print(f\"{dataset_name}: {dataset.shape[0]} rows, {dataset.shape[1]} columns\\n\")\n        \n        # Dataset Summary\n        print_heading(f\"{dataset_name} Summary\")\n        print(dataset.describe())\n        \n        # Null values\n        print_heading(f\"Null Values in {dataset_name}\")\n        null_count = dataset.isnull().sum()\n        null_percentage = (null_count / len(dataset)) * 100\n        null_summary = pd.DataFrame({\n            \"Null Count\": null_count[null_count > 0],\n            \"Null Percentage (%)\": null_percentage[null_percentage > 0]\n        })\n        if null_count.sum() == 0:\n            print(f\"No null values in the {dataset_name}.\")\n        else:\n            print(null_summary)\n        \n        # Duplicate rows\n        print_heading(f\"Duplicate Values in {dataset_name}\")\n        duplicates = dataset.duplicated().sum()\n        print(f\"{dataset_name}: {duplicates} duplicate rows\\n\")\n\n    except Exception as e:\n        print_error(str(e))\n\n# Function to analyze both train and test datasets\ndef analyze_datasets(train_dataset, test_dataset, n_top=5):\n    try:\n        # Display top rows of train dataset\n        print_heading(f\"Top {n_top} Rows of Training Dataset\")\n        print(train_dataset.head(n_top))\n        \n        # Display top rows of test dataset\n        print_heading(f\"Top {n_top} Rows of Test Dataset\")\n        print(test_dataset.head(n_top))\n        \n        # Display dataset summaries\n        display_dataset_summary(train_dataset, \"Training Dataset\")\n        display_dataset_summary(test_dataset, \"Test Dataset\")\n\n    except Exception as e:\n        print_error(str(e))\n\n# Function to print unique values in columns\ndef print_unique_values(dataset):\n    try:\n        print_heading(\"Unique Values in Dataset\")\n        \n        for column in dataset.columns:\n            unique_values = dataset[column].unique()[:7]  # Limiting to first 7 unique values\n            data_type = dataset[column].dtype\n            print(f\"Column: {column}, Data Type: {data_type}, Unique Values: {', '.join(map(str, unique_values))}\")\n    \n    except Exception as e:\n        print_error(str(e))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:40:22.564973Z","iopub.execute_input":"2024-12-11T12:40:22.565304Z","iopub.status.idle":"2024-12-11T12:40:22.578442Z","shell.execute_reply.started":"2024-12-11T12:40:22.565272Z","shell.execute_reply":"2024-12-11T12:40:22.577112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"analyze_datasets(train, test)  # Correct function call\nprint_unique_values(train) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:40:22.580549Z","iopub.execute_input":"2024-12-11T12:40:22.581006Z","iopub.status.idle":"2024-12-11T12:40:22.952830Z","shell.execute_reply.started":"2024-12-11T12:40:22.580968Z","shell.execute_reply":"2024-12-11T12:40:22.951625Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<p style=\"font-family: 'Amiri'; font-size: 3rem; color: white; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #34495e; padding: 20px; border-radius: 20px; border: 7px solid #e74c3c; width:95%\">\r\n   Data Visualization˚\r\n</p>\r\n","metadata":{}},{"cell_type":"code","source":"def replace_nulls_with_default(df, float_default=0.0, object_default=\"Unknown\"):\n    \"\"\"\n    Replace null values in a DataFrame with default values based on data type.\n\n    Parameters:\n        df (pd.DataFrame): The DataFrame to process.\n        float_default (float, optional): Default value for float columns. Default is 0.0.\n        object_default (str, optional): Default value for object (string) columns. Default is \"Unknown\".\n    \n    Returns:\n        pd.DataFrame: DataFrame with nulls replaced.\n    \"\"\"\n    # Replace nulls in float columns\n    df.update(df.select_dtypes(include=['float']).fillna(float_default))\n    # Replace nulls in object columns\n    df.update(df.select_dtypes(include=['object']).fillna(object_default))\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:40:22.954140Z","iopub.execute_input":"2024-12-11T12:40:22.954499Z","iopub.status.idle":"2024-12-11T12:40:22.960919Z","shell.execute_reply.started":"2024-12-11T12:40:22.954463Z","shell.execute_reply":"2024-12-11T12:40:22.959620Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = replace_nulls_with_default(train, float_default=-1.0, object_default=\"Missing\")\ntest = replace_nulls_with_default(test, float_default=-1.0, object_default=\"Missing\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:40:22.962298Z","iopub.execute_input":"2024-12-11T12:40:22.962672Z","iopub.status.idle":"2024-12-11T12:40:23.238170Z","shell.execute_reply.started":"2024-12-11T12:40:22.962636Z","shell.execute_reply":"2024-12-11T12:40:23.237022Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_object_to_category(df):\n    \"\"\"\n    Converts all columns with object data type to category data type in a DataFrame.\n\n    Parameters:\n        df (pd.DataFrame): The DataFrame to process.\n    \n    Returns:\n        pd.DataFrame: The updated DataFrame with object columns converted to category.\n    \"\"\"\n    for col in df.select_dtypes(include=['object']).columns:\n        df[col] = df[col].astype('category')\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:40:23.239611Z","iopub.execute_input":"2024-12-11T12:40:23.239986Z","iopub.status.idle":"2024-12-11T12:40:23.245576Z","shell.execute_reply.started":"2024-12-11T12:40:23.239949Z","shell.execute_reply":"2024-12-11T12:40:23.244448Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = convert_object_to_category(train)\ntest = convert_object_to_category(test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:40:23.247098Z","iopub.execute_input":"2024-12-11T12:40:23.247695Z","iopub.status.idle":"2024-12-11T12:40:23.378618Z","shell.execute_reply.started":"2024-12-11T12:40:23.247661Z","shell.execute_reply":"2024-12-11T12:40:23.377451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def print_value_counts_less_than(df, threshold=50):\n    for col in df.columns:\n        value_counts = df[col].value_counts()\n        if len(value_counts) < threshold:\n            print(f\"Value counts for column '{col}':\")\n            for val, count in value_counts.items():\n                print(f\"    {val}: {count}\")\n            print()\n\nprint_value_counts_less_than(train, threshold=30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:40:23.379953Z","iopub.execute_input":"2024-12-11T12:40:23.380300Z","iopub.status.idle":"2024-12-11T12:40:23.442181Z","shell.execute_reply.started":"2024-12-11T12:40:23.380264Z","shell.execute_reply":"2024-12-11T12:40:23.440961Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_low_value_count_columns_pie(df):\n    # Identify columns with less than 20 unique values\n    low_value_count_columns = [col for col in df.columns if df[col].nunique() < 20]\n    \n    # Custom color palette for the pie chart\n    custom_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n    \n    # Iterate through each of the identified columns\n    for col in low_value_count_columns:\n        # Get the value counts of the column\n        value_counts = df[col].value_counts()\n        \n        # Plotting a pie chart with annotations for each slice\n        plt.figure(figsize=(8, 8))\n        wedges, texts, autotexts = plt.pie(value_counts, \n                                           labels=value_counts.index, \n                                           autopct='%1.1f%%', \n                                           startangle=90, \n                                           colors=custom_colors[:len(value_counts)])\n        \n        # Adding a title and customizing the plot\n        plt.title(f'Distribution of {col}', fontsize=16, fontweight='bold', color='darkblue')\n        \n        # Customize annotations (the text inside the slices)\n        for text in texts:\n            text.set(fontsize=12, color='black', fontweight='bold')\n        \n        for autotext in autotexts:\n            autotext.set(fontsize=10, color='white', fontweight='bold')\n        \n        # Show plot\n        plt.show()\n\n# Example usage\nplot_low_value_count_columns_pie(train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:40:23.443410Z","iopub.execute_input":"2024-12-11T12:40:23.443745Z","iopub.status.idle":"2024-12-11T12:40:32.471797Z","shell.execute_reply.started":"2024-12-11T12:40:23.443711Z","shell.execute_reply":"2024-12-11T12:40:32.470699Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef separate_int_columns(df):\n    return df.select_dtypes(include=['int'])\n\nnumerical_columns = [col for col in train.select_dtypes(include=['number']).columns if col != 'Target']\n\n# Correlation Heatmap\ncorrelation_matrix = train[numerical_columns].corr()\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, linewidths=0.5, fmt='.2f', cbar=True)\nplt.title('Correlation Heatmap of Numerical Columns', fontsize=16, fontweight='bold', color='darkblue')\nplt.show()\n\n# Individual Histograms for Each Numerical Column\nfig, axes = plt.subplots(nrows=len(numerical_columns), ncols=1, figsize=(10, len(numerical_columns)*4))\naxes = axes.flatten()\n\nfor i, col in enumerate(numerical_columns):\n    sns.histplot(train[col], kde=True, ax=axes[i], color='skyblue', bins=20)\n    axes[i].set_title(f'{col} Distribution', fontsize=14, fontweight='bold', color='darkblue')\n    axes[i].tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:40:32.473138Z","iopub.execute_input":"2024-12-11T12:40:32.473586Z","iopub.status.idle":"2024-12-11T12:40:43.892233Z","shell.execute_reply.started":"2024-12-11T12:40:32.473538Z","shell.execute_reply":"2024-12-11T12:40:43.890517Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<p style=\"font-family: 'Amiri'; font-size: 3rem; color: white; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #34495e; padding: 20px; border-radius: 20px; border: 7px solid #e74c3c; width:95%\">\n   ML Algorithms \n</p>","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostRegressor, Pool\nfrom sklearn.model_selection import StratifiedKFold\n\n# Custom function to compute the Stratified Concordance Index (C-index)\ndef stratified_c_index(y_true, y_pred, groups):\n    \"\"\"\n    Compute the Stratified Concordance Index (C-index) for the predicted values.\n    \n    Args:\n        y_true (array): True values of the target variable.\n        y_pred (array): Predicted values of the target variable.\n        groups (array): Grouping variable for stratified computation (e.g., race groups).\n        \n    Returns:\n        float: Stratified C-index score.\n    \"\"\"\n    unique_groups = np.unique(groups)\n    c_indices = []\n\n    for group in unique_groups:\n        mask = groups == group\n        if sum(mask) > 1:  # Only consider groups with more than one member\n            y_true_group = y_true[mask]\n            y_pred_group = y_pred[mask]\n            concordant = 0\n            permissible = 0\n\n            for i in range(len(y_true_group)):\n                for j in range(i + 1, len(y_true_group)):\n                    if y_true_group[i] != y_true_group[j]:\n                        permissible += 1\n                        if (y_pred_group[i] > y_pred_group[j] and y_true_group[i] > y_true_group[j]) or \\\n                           (y_pred_group[i] < y_pred_group[j] and y_true_group[i] < y_true_group[j]):\n                            concordant += 1\n\n            c_indices.append(concordant / permissible if permissible > 0 else 0)\n\n    c_indices = np.array(c_indices)\n    return np.mean(c_indices) - np.std(c_indices)\n\n# 1. Prepare the dataset (assuming df_tr is predefined)\nX = train.drop(columns=['target'], axis=1)  # Features\ny = train['target']  # Target variable\nrace_groups = train['race_group']  # Grouping variable for stratified C-index\n\n# 2. Identify categorical features for CatBoost\ncat_features = list(X.select_dtypes(include=['object', 'category']).columns)\n\n# 3. Initialize K-Fold Cross Validation\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfinal_predictions = np.zeros(len(X))  # To store final predictions\n\n# 4. Train and validate the model using Stratified K-Fold\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X, race_groups)):\n    print(f\"Training on Fold {fold + 1}\")\n    \n    # Split data into train and validation sets\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    race_val = race_groups.iloc[val_idx]\n\n    # Prepare Pool datasets for CatBoost\n    train_pool = Pool(X_train, y_train, cat_features=cat_features)\n    val_pool = Pool(X_val, y_val, cat_features=cat_features)\n\n    # Initialize CatBoostRegressor model\n    model = CatBoostRegressor(\n        iterations=1000,\n        learning_rate=0.05,\n        depth=6,\n        l2_leaf_reg=3,\n        loss_function='RMSE',\n        random_seed=42,\n        verbose=100\n    )\n\n    # Train the model with early stopping\n    model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=50)\n\n    # Predict and compute the Stratified C-index\n    y_val_pred = model.predict(X_val)\n    fold_score = stratified_c_index(y_val.values, y_val_pred, race_val.values)\n    print(f\"Stratified C-Index for Fold {fold + 1}: {fold_score:.4f}\")\n\n    # Store predictions\n    final_predictions[val_idx] = y_val_pred\n\n# 5. Compute the Overall Stratified C-index\noverall_score = stratified_c_index(y.values, final_predictions, race_groups.values)\nprint(f\"Overall Stratified C-Index: {overall_score:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:40:43.893811Z","iopub.execute_input":"2024-12-11T12:40:43.894273Z","iopub.status.idle":"2024-12-11T12:48:27.706987Z","shell.execute_reply.started":"2024-12-11T12:40:43.894234Z","shell.execute_reply":"2024-12-11T12:48:27.705610Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred = model.predict(test)\n\n\nprint(pred)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:48:27.708357Z","iopub.execute_input":"2024-12-11T12:48:27.708684Z","iopub.status.idle":"2024-12-11T12:48:27.719566Z","shell.execute_reply.started":"2024-12-11T12:48:27.708647Z","shell.execute_reply":"2024-12-11T12:48:27.718296Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"solution.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:48:27.721259Z","iopub.execute_input":"2024-12-11T12:48:27.722268Z","iopub.status.idle":"2024-12-11T12:48:27.740212Z","shell.execute_reply.started":"2024-12-11T12:48:27.722215Z","shell.execute_reply":"2024-12-11T12:48:27.738920Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"solution['prediction'] = pred\nsolution.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:48:27.741827Z","iopub.execute_input":"2024-12-11T12:48:27.742238Z","iopub.status.idle":"2024-12-11T12:48:27.750813Z","shell.execute_reply.started":"2024-12-11T12:48:27.742201Z","shell.execute_reply":"2024-12-11T12:48:27.749637Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"solution.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:48:27.752151Z","iopub.execute_input":"2024-12-11T12:48:27.752519Z","iopub.status.idle":"2024-12-11T12:48:27.767535Z","shell.execute_reply.started":"2024-12-11T12:48:27.752484Z","shell.execute_reply":"2024-12-11T12:48:27.766266Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Thank you","metadata":{}}]}