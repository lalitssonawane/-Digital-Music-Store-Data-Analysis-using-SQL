{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":119261,"databundleVersionId":14363498,"sourceType":"competition"}],"dockerImageVersionId":31235,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üß† Teach Gemma 3 to Reason: The \"Plan-Evaluate-Execute\" Loop (Final Research Version)\n\n**Objective:** Train Gemma 3 1B to simulate System 2 thinking (deep reasoning) using GRPO.\n\n**Fixed Issues in this Version:**\n* ‚úÖ **Fixed Mode Collapse:** Solved the 0% accuracy issue where the model only generated empty XML tags.\n* ‚úÖ **Fixed Cold Start:** Injected a \"One-Shot\" example into the training data so the model knows *what* to write.\n* ‚úÖ **Fixed Infinite Loops:** Added `repetition_penalty` to inference to stop `</answer></answer>...` spam.\n* ‚úÖ **Robust Config:** Patched `NoneType` errors in the Tunix configuration loader.\n\n**Hardware:** TPU VM v3-8 (Recommended) or GPU T4 x2.\n\n---","metadata":{}},{"cell_type":"markdown","source":"### **Step 1: Environment Setup**\n*Installs the correct JAX/Tunix ecosystem. **Restart Kernel** after running this.*","metadata":{}},{"cell_type":"code","source":"# --- INSTALLATION ---\nimport os\nimport sys\n\nprint(\"Installing Research Environment...\")\n\n# 1. Clean Slate\n!pip uninstall -y jax jaxlib flax tunix qwix libtpu-nightly\n\n# 2. Install JAX (Detects TPU or GPU automatically)\ntry:\n    if os.environ.get(\"TPU_NAME\"):\n        !pip install -U \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n    else:\n        !pip install -U \"jax[cuda12]\"\nexcept:\n    pass\n\n# 3. Install Tunix Ecosystem\n!pip install git+https://github.com/google/tunix.git\n!pip install git+https://github.com/google/qwix.git\n!pip install git+https://github.com/google/flax.git\n\n# 4. Dependencies\n!pip install -q \"numpy>2\" tensorflow tensorflow_datasets tensorboardX transformers grain huggingface_hub datasets\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"‚ö†Ô∏è  ACTION REQUIRED: RESTART KERNEL NOW  ‚ö†Ô∏è\")\nprint(\"Go to 'Run' > 'Restart Kernel' or 'Runtime' > 'Restart Session'\")\nprint(\"=\"*50)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Step 2: Initialization & Config Injection**\n*Loads the model and applies the critical configuration patches.*","metadata":{}},{"cell_type":"code","source":"import os\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\nimport qwix\nfrom huggingface_hub import login, snapshot_download\nimport json\nimport inspect\nfrom tunix.models.gemma3 import model as gemma_lib\nfrom tunix.models.gemma3 import params_safetensors as params_safetensors_lib\n\n# --- CHECK HARDWARE ---\nprint(f\"‚úÖ JAX Version: {jax.__version__}\")\ntry:\n    devices = jax.devices()\n    print(f\"‚úÖ Detected Devices: {len(devices)} ({devices[0].platform})\")\n    IS_TPU = \"tpu\" in str(devices[0]).lower()\nexcept Exception as e:\n    print(f\"‚ùå Hardware Error: {e}\")\n    IS_TPU = False\n\n# --- AUTH ---\nfrom kaggle_secrets import UserSecretsClient\ntry:\n    user_secrets = UserSecretsClient()\n    HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n    login(token=HF_TOKEN)\nexcept:\n    print(\"‚ö†Ô∏è Secrets not found. Attempting manual login...\")\n    login()\n\n# --- MODEL CONFIGURATION ---\nMODEL_ID = \"google/gemma-3-1b-it\"\nlocal_path = snapshot_download(repo_id=MODEL_ID, ignore_patterns=[\"*.pth\"], token=HF_TOKEN)\n\nwith open(os.path.join(local_path, \"config.json\"), \"r\") as f:\n    hf_config = json.load(f)\n\n# Robust Config Mapping\nmappings = {\n    \"num_embed\": \"vocab_size\", \"embed_dim\": \"hidden_size\", \"hidden_dim\": \"intermediate_size\",\n    \"num_heads\": \"num_attention_heads\", \"num_kv_heads\": \"num_key_value_heads\",\n    \"num_layers\": \"num_hidden_layers\", \"head_dim\": \"head_dim\", \"sliding_window_size\": \"sliding_window\",\n    \"rope_base_frequency\": \"rope_theta\"\n}\n\nconfig_args = {}\ntry:\n    valid_keys = set(inspect.signature(gemma_lib.ModelConfig).parameters.keys())\nexcept:\n    valid_keys = set(mappings.keys())\n\nfor tunix_k, hf_k in mappings.items():\n    if tunix_k in valid_keys and hf_k in hf_config:\n        config_args[tunix_k] = hf_config[hf_k]\n\nmodel_config = gemma_lib.ModelConfig(**config_args)\n\n# --- ATTRIBUTE INJECTION (Fixes NoneType Errors) ---\ntry:\n    if not hasattr(model_config, 'query_pre_attn_scalar') or model_config.query_pre_attn_scalar is None:\n        object.__setattr__(model_config, 'query_pre_attn_scalar', 256.0)\nexcept AttributeError: pass\n\ntry:\n    if not hasattr(model_config, 'sliding_window_size') or model_config.sliding_window_size is None:\n        object.__setattr__(model_config, 'sliding_window_size', 4096)\nexcept AttributeError: pass\n\n# --- MESH SETUP ---\nprint(\"Initializing Mesh...\")\nif IS_TPU:\n    # FSDP across all cores for TPU v3-8\n    mesh = jax.make_mesh((len(jax.devices()), 1), axis_names=(\"fsdp\", \"tp\"))\nelse:\n    # GPU setup\n    mesh = jax.make_mesh((1, len(jax.devices())), axis_names=(\"fsdp\", \"tp\"))\n\nwith mesh:\n    base_model = params_safetensors_lib.create_model_from_safe_tensors(local_path, model_config, mesh)\n    \n    lora_provider = qwix.LoraProvider(\n        module_path=\".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|.*attn_vec_einsum\",\n        rank=32,\n        alpha=64.0,\n    )\n    \n    model_input = base_model.get_model_input()\n    lora_policy = qwix.apply_lora_to_model(base_model, lora_provider, **model_input)\n    \n    state = nnx.state(lora_policy)\n    sharded_state = jax.lax.with_sharding_constraint(state, nnx.get_partition_spec(state))\n    nnx.update(lora_policy, sharded_state)\n\nprint(\"‚úÖ Policy Model Initialized (Attributes Patched).\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Step 3: Data Pipeline (The \"Cold Start\" Fix)**\n**CRITICAL UPDATE:** We inject a \"One-Shot\" example into every prompt. This prevents the model from generating empty tags by showing it *exactly* what filled tags look like.","metadata":{}},{"cell_type":"code","source":"import tensorflow_datasets as tfds\nimport grain.python as grain\nimport re\n\n# 1. THE GOLDEN EXAMPLE (Teaches the model HOW to reason)\nONE_SHOT_EXAMPLE = (\n    \"Problem: I have 2 apples and buy 2 more. How many total?\\n\"\n    \"<end_of_turn>\\n<start_of_turn>model\\n\"\n    \"<brainstorm> 1. Simple addition: 2+2. 2. Counting on fingers. </brainstorm>\\n\"\n    \"<evaluate> Addition is faster and standard. </evaluate>\\n\"\n    \"<solve> 2 + 2 = 4. </solve>\\n\"\n    \"<answer> 4 </answer><end_of_turn>\\n\"\n)\n\nSYSTEM_PROMPT = (\n    \"You are an advanced reasoning engine. For every problem, you must follow this strict XML structure:\\n\"\n    \"1. <brainstorm> List 2-3 distinct approaches. </brainstorm>\\n\"\n    \"2. <evaluate> Critique the approaches. </evaluate>\\n\"\n    \"3. <solve> Solve step-by-step. </solve>\\n\"\n    \"4. <answer> Put the final number here. </answer>\"\n)\n\ndef get_structured_dataset(split=\"train\"):\n    ds = tfds.load(\"gsm8k\", split=split, as_supervised=False)\n    ds_list = list(ds.as_numpy_iterator())\n    \n    def format_fn(ex):\n        q = ex['question'].decode('utf-8')\n        a = ex['answer'].decode('utf-8').split(\"####\")[-1].strip()\n        \n        # INJECT EXAMPLE HERE\n        full_prompt = (\n            f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n\"\n            f\"Example:\\n{ONE_SHOT_EXAMPLE}\\n\\n\"\n            f\"Problem: {q}<end_of_turn>\\n<start_of_turn>model\\n\"\n        )\n        return {\"prompts\": full_prompt, \"question\": q, \"answer\": a}\n    \n    # Batch size optimization (8 for TPU, 1 for GPU)\n    batch_size = 8 if IS_TPU else 1\n    return grain.MapDataset.source(ds_list).map(format_fn).shuffle(seed=42).batch(batch_size)\n\ntrain_ds = get_structured_dataset(\"train\").repeat(100)\nprint(\"‚úÖ Dataset Ready with One-Shot Injection.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Step 4: Reward Engineering (The \"Anti-Hacking\" Fix)**\nWe introduce penalties to stop the model from gaming the system.\n* **Content Penalty:** `-5.0` points if `<answer>` tags are empty.\n* **Strict Correctness:** `+10.0` points for correct math (jackpot).","metadata":{}},{"cell_type":"code","source":"# 1. CONTENT PENALTY (Prevents Empty Tags)\ndef content_presence_reward(prompts, completions, **kwargs):\n    rewards = []\n    for c in completions:\n        try:\n            if \"<answer>\" in c and \"</answer>\" in c:\n                content = c.split(\"<answer>\")[1].split(\"</answer>\")[0].strip()\n                if len(content) > 0:\n                    rewards.append(0.5)  # Reward for writing something\n                else:\n                    rewards.append(-5.0) # PENALTY for empty tags\n            else:\n                rewards.append(0.0)\n        except:\n            rewards.append(0.0)\n    return rewards\n\n# 2. STRICT CORRECTNESS (High Stakes)\ndef correctness_reward_strict(prompts, completions, answer, **kwargs):\n    rewards = []\n    for c, gt in zip(completions, answer):\n        try:\n            if \"<answer>\" in c:\n                pred = c.split(\"<answer>\")[1].split(\"</answer>\")[0].strip()\n                pred_clean = re.sub(r\"[^0-9\\.\\-]\", \"\", pred)\n                gt_clean = re.sub(r\"[^0-9\\.\\-]\", \"\", gt)\n                \n                if pred_clean == gt_clean and len(pred_clean) > 0:\n                    rewards.append(10.0) # JACKPOT\n                else:\n                    rewards.append(0.0)\n            else:\n                rewards.append(0.0)\n        except:\n            rewards.append(0.0)\n    return rewards\n\n# 3. FORMAT GATE (Weak Nudge)\ndef format_reward_weak(prompts, completions, **kwargs):\n    rewards = []\n    required_tags = [\"<brainstorm>\", \"<evaluate>\", \"<solve>\", \"<answer>\"]\n    for c in completions:\n        if all(tag in c for tag in required_tags):\n            rewards.append(0.1)\n        else:\n            rewards.append(-1.0) # Penalty for breaking structure\n    return rewards\n\nprint(\"‚úÖ Rewards Ready: Penalties Active.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Step 5: GRPO Training**\nTraining with `beta=0.1` to prevent the model from drifting too far (Mode Collapse protection).","metadata":{}},{"cell_type":"code","source":"import optax\nfrom tunix.rl import rl_cluster as rl_cluster_lib\nfrom tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\nfrom tunix.rl.rollout import base_rollout\nfrom tunix.generate import tokenizer_adapter as tokenizer_lib\n\nGEMMA_TOKENIZER_PATH = \"gs://gemma-data/tokenizers/tokenizer_gemma3.model\"\ntokenizer = tokenizer_lib.Tokenizer(tokenizer_path=GEMMA_TOKENIZER_PATH)\nCHECKPOINT_DIR = os.path.abspath(\"checkpoints/grpo_final\")\n\ncluster_config = rl_cluster_lib.ClusterConfig(\n    role_to_mesh={\n        rl_cluster_lib.Role.ACTOR: mesh,\n        rl_cluster_lib.Role.REFERENCE: mesh,\n        rl_cluster_lib.Role.ROLLOUT: mesh,\n    },\n    training_config=rl_cluster_lib.RLTrainingConfig(\n        actor_optimizer=optax.adamw(learning_rate=2e-6),\n        max_steps=300,\n        mini_batch_size=8 if IS_TPU else 1,\n        checkpoint_root_directory=CHECKPOINT_DIR,\n        eval_every_n_steps=1000, \n    ),\n    rollout_config=base_rollout.RolloutConfig(\n        max_tokens_to_generate=512,\n        temperature=0.85,\n        eos_tokens=[tokenizer.eos_id()],\n    ),\n)\n\nrl_cluster = rl_cluster_lib.RLCluster(\n    actor=lora_policy,\n    reference=base_model,\n    tokenizer=tokenizer,\n    cluster_config=cluster_config,\n)\n\ntrainer = GRPOLearner(\n    rl_cluster=rl_cluster,\n    reward_fns=[format_reward_weak, content_presence_reward, correctness_reward_strict],\n    algo_config=GRPOConfig(\n        num_generations=8 if IS_TPU else 4,\n        beta=0.1, # Increased Beta to stabilize logic\n    ),\n)\n\nprint(\"üöÄ Starting Final GRPO Training Loop...\")\nwith mesh:\n    trainer.train(train_ds, None)\nprint(\"‚úÖ Training Complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Step 6: Inference & Evaluation (Loop Fix)**\nUsing `repetition_penalty=1.2` to ensure the model stops generating tags and finishes the answer.","metadata":{}},{"cell_type":"code","source":"from tunix.generate import sampler as sampler_lib\nimport tunix.generate\nimport inspect\n\n# --- ROBUST CACHE CONFIG --- #\nCacheConfigClass = tunix.generate.sampler.CacheConfig\nvalid_keys = set(inspect.signature(CacheConfigClass).parameters.keys())\ncache_args = {\n    \"cache_size\": 2048, \"num_layers\": model_config.num_layers,\n    \"num_kv_heads\": model_config.num_kv_heads, \"head_dim\": model_config.head_dim,\n    \"dtype\": jnp.bfloat16\n}\nfinal_args = {k: v for k, v in cache_args.items() if k in valid_keys}\ncache_cfg = CacheConfigClass(**final_args)\n\n# --- ROBUST SAMPLER INIT --- #\ntry:\n    sampler = sampler_lib.Sampler(module=lora_policy, tokenizer=tokenizer, cache_config=cache_cfg)\nexcept TypeError:\n    sampler = sampler_lib.Sampler(lora_policy, tokenizer, cache_cfg)\n\n# --- TEST PROMPT --- #\nprompt_text = (\n    f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n\"\n    f\"Example:\\n{ONE_SHOT_EXAMPLE}\\n\\n\"\n    \"Problem: A store sells apples for $2 and oranges for $3. Alice buys 5 fruits and spends $12. How many apples did she buy?\"\n    \"<end_of_turn>\\n<start_of_turn>model\\n\"\n)\n\nprint(\"üß† Generating with Repetition Penalty (Anti-Loop)...\")\nwith mesh:\n    try:\n        # The Repetition Penalty is key here\n        outputs = sampler(input_strings=[prompt_text], max_generation_steps=1024, temperature=0.7, repetition_penalty=1.2)\n    except TypeError:\n        # Fallback for older Tunix versions without rep_penalty kwarg\n        outputs = sampler([prompt_text], 1024, 0.7)\n\n# --- PARSING --- #\noutput_text = outputs.text[0]\nprint(\"=\"*60)\nprint(f\"üìù RAW OUTPUT:\\n{output_text}\")\nprint(\"=\"*60)\n\nif \"<answer>\" in output_text:\n    ans = output_text.split(\"<answer>\")[1].split(\"</answer>\")[0]\n    print(f\"üéØ FINAL ANSWER EXTRACTED: {ans}\")\nelse:\n    print(\"‚ùå No Answer Tag Found (Check logs).\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}